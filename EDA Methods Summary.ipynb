{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACE framework \n",
    "| Stage     | Description                                           | Key Questions to Ask                        |\n",
    "|-----------|-------------------------------------------------------|--------------------------------------------|\n",
    "| Plan      | Define project scope and informational needs         | - What are the project goals?              |\n",
    "|           | Prepare for success                                   | - What strategies are needed?              |\n",
    "|           |                                                       | - What are the business impacts?           |\n",
    "| Analyze   | Engage with the data                                  | - How can I acquire and clean the data?    |\n",
    "|           | Prepare data for analysis                             | - What insights can EDA uncover?           |\n",
    "|           | Collaborate with stakeholders                         | - Which areas are worth pursuing in detail? |\n",
    "| Construct | Pursue limited subset of insights from EDA           | - How can I build and revise models?        |\n",
    "|           | Work with data professionals                          | - What relationships can I uncover?         |\n",
    "|           |                                                       | - What statistical inferences can I make?   |\n",
    "| Execute   | Share analysis results and collaboration             | - How can I present findings effectively?   |\n",
    "|           | Present findings to internal and external stakeholders | - What recommendations can I provide?       |\n",
    "|           | Revisit planning and analysis stages as needed       | - How can I incorporate feedback?           |\n",
    "\n",
    "These key questions will guide you through each stage of the PACE framework, helping you stay focused on the necessary tasks and ensure that your data analytics projects are well-planned, executed, and communicated with stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration\n",
    "\n",
    "Here's a cheat sheet with the top 6 methods for basic data exploration, along with the description and code to implement them:\n",
    "\n",
    "| Method                   | Description                                                                                                 | Code                                                                     |\n",
    "|--------------------------|-------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| Overview of the Dataset  | Get a high-level overview of the dataset, including the number of rows and columns, data types, and summary statistics | ```df.info()``` or ```df.describe()```                                 |\n",
    "| Missing Values           | Check for missing values in the dataset and their distribution                                               | ```df.isnull().sum()``` or ```df.isnull().sum()/len(df)```              |\n",
    "| Categorical Variables    | Explore categorical variables and their unique values                                                        | ```df.select_dtypes(include='object').nunique()```                      |\n",
    "| Numeric Variables        | Explore numeric variables and their summary statistics                                                        | ```df.select_dtypes(include=['int64', 'float64']).describe()```         |\n",
    "| Distribution Visualization | Visualize the distribution of numeric variables using histograms or density plots                             | ```import seaborn as sns```<br>```import matplotlib.pyplot as plt```<br>```sns.histplot(df['column_name'])``` or ```sns.kdeplot(df['column_name'])``` |\n",
    "| Categorical Visualization| Visualize the distribution of categorical variables using bar plots or count plots                            | ```import seaborn as sns```<br>```import matplotlib.pyplot as plt```<br>```sns.countplot(x='column_name', data=df)``` or ```sns.barplot(x='column_name', y='target_column', data=df)```|\n",
    "\n",
    "These methods are useful for gaining insights into the data, understanding its structure, and identifying potential issues such as missing values and outliers. Remember to customize your data exploration based on the specific characteristics of your dataset and the questions you want to answer. Visualization tools like Seaborn and Matplotlib can be powerful allies in this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "Here's a cheat sheet with the top 6 methods to deal with missing values, along with the description and code to implement them:\n",
    "\n",
    "| Method                | Description                                                                                       | Code                                                                      |\n",
    "|-----------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| Drop Missing Data     | Remove rows or columns with missing values                                                        | ```df.dropna(axis=0)``` or ```df.dropna(axis=1)```                       |\n",
    "| Mean/Median Imputation | Replace missing values with the mean or median of the respective column                           | ```df['column_name'].fillna(df['column_name'].mean())``` or ```df['column_name'].fillna(df['column_name'].median())``` |\n",
    "| Mode Imputation       | Replace missing values with the mode (most frequent value) of the respective column                | ```df['column_name'].fillna(df['column_name'].mode()[0])```              |\n",
    "| Forward Fill (FFill)  | Propagate the last valid observation forward along the column to fill missing values               | ```df.fillna(method='ffill')```                                           |\n",
    "| Backward Fill (BFill) | Propagate the next valid observation backward along the column to fill missing values              | ```df.fillna(method='bfill')```                                           |\n",
    "| Multiple Imputation   | Use predictive modeling to estimate missing values based on other features and multiple iterations | You can use libraries like MICE (Multivariate Imputation by Chained Equations) in Python to perform multiple imputation. |\n",
    "\n",
    "Choose the appropriate method based on the nature and amount of missing data in your dataset. Keep in mind that the choice of missing data imputation method can impact the performance of your machine learning model, so it's essential to evaluate the performance of the model after applying the imputation technique. Additionally, be cautious when using imputation methods as they can introduce bias or affect the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features\n",
    "\n",
    "Sure! Here's a cheat sheet with the top 6 encoding techniques along with the code to implement them:\n",
    "\n",
    "| Encoding Technique   | Description                                                                                   | Code                                                                                                  |\n",
    "|----------------------|-----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| One-Hot Encoding     | Create binary columns for each category, representing categorical variables as numerical values | ```pd.get_dummies(df, columns=['categorical_column'])```                                              |\n",
    "| Target Encoding      | Replace categories with the mean target value                                                  | ```df['categorical_column'] = df.groupby('categorical_column')['target_column'].transform('mean')``` |\n",
    "| Frequency Encoding   | Replace categories with their frequency in the dataset                                        | ```df['categorical_column'] = df['categorical_column'].map(df['categorical_column'].value_counts())``` |\n",
    "| Binary Encoding      | Convert categories into binary representations                                                | ```import category_encoders as ce; encoder = ce.BinaryEncoder(cols=['categorical_column']); df = encoder.fit_transform(df)``` |\n",
    "| Hash Encoding        | Reduce dimensionality of the encoded data                                                      | ```import category_encoders as ce; encoder = ce.HashingEncoder(cols=['categorical_column']); df = encoder.fit_transform(df)``` |\n",
    "| Leave-One-Out Encoding | Useful for small data with high-cardinality categorical variables; less prone to overfitting  | ```import category_encoders as ce; encoder = ce.LeaveOneOutEncoder(cols=['categorical_column']); df = encoder.fit_transform(df, target_column)``` |\n",
    "\n",
    "Please note that for some of these encoding techniques, you may need to install the `category_encoders` library, which provides additional encoding methods like Binary Encoding, Hash Encoding, and Leave-One-Out Encoding. You can install it using `pip install category_encoders`.\n",
    "\n",
    "Remember to replace `'categorical_column'` with the name of the categorical column you want to encode and `'target_column'` with the name of your target variable.\n",
    "\n",
    "Make sure to experiment with these encoding techniques and evaluate their impact on your model's performance using appropriate evaluation metrics. The effectiveness of each technique may vary depending on the dataset and the machine learning algorithm being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the appropriate encoding technique\n",
    "\n",
    "1. **One-Hot Encoding**: Use this technique for categorical features with low cardinality (a small number of unique values) and when there is no inherent ordinal relationship between the categories. One-hot encoding creates binary columns for each category, which can work well for nominal variables.\n",
    "\n",
    "2. **Target Encoding**: This technique is suitable for high cardinality categorical features (many unique values) and when the target variable shows a clear relationship with the categories. Target encoding replaces each category with the mean target value for that category. It can be useful when you have imbalanced data or when there is a strong correlation between the category and the target.\n",
    "\n",
    "3. **Frequency Encoding**: Frequency encoding is useful for high cardinality categorical features when you want to capture the frequency of each category in the dataset. It replaces each category with its frequency in the dataset.\n",
    "\n",
    "4. **Binary Encoding**: Binary encoding is suitable for both low and high cardinality categorical features. It creates binary representations of the categories, reducing the dimensionality of the encoded data.\n",
    "\n",
    "5. **Hash Encoding**: Hash encoding is also useful for both low and high cardinality categorical features. It reduces dimensionality like binary encoding but may result in hash collisions, which can lead to information loss.\n",
    "\n",
    "6. **Leave-One-Out Encoding**: This technique is useful for high cardinality categorical features with a small dataset. It reduces the risk of overfitting compared to target encoding and can be beneficial when you have limited data.\n",
    "\n",
    "Based on the information provided in the table, you can consider the following encoding choices:\n",
    "\n",
    "- For 'Suburb' and 'SellerG', which have high cardinality, you can use Target Encoding.\n",
    "- For 'Address', you may also use Target Encoding due to its high cardinality, but it might be more efficient to group similar addresses or use a different technique, depending on your specific use case.\n",
    "- For 'Type', 'Method', 'CouncilArea', and 'Regionname', which have low cardinality, you can use One-Hot Encoding.\n",
    "- For 'Date', which has a moderate number of unique values, you can consider using Target Encoding or other time-related encoding techniques if the date has a clear temporal relationship with the target variable.\n",
    "\n",
    "Keep in mind that these recommendations are general guidelines, and you should always validate the encoding techniques by evaluating their impact on your model's performance through cross-validation and other performance metrics. Additionally, you can experiment with different techniques and combinations to find the best encoding strategy for your particular dataset and machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling skewed data and outliers\n",
    "\n",
    "Here's a table summarizing different methods for handling skewed data and outliers, along with their use cases:\n",
    "\n",
    "| Method                       | Description                                                                                                                                                                   | Use Case                                                                                                      |\n",
    "|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| Log Transformation           | Apply a logarithmic function to data to reduce the impact of positive skewness.                                                                                              | When dealing with data that follows a power-law distribution or has a long tail of high values.               |\n",
    "| Winsorization                | Cap or clip extreme values to a specified percentile to reduce the impact of outliers.                                                                                       | When you want to mitigate the effect of outliers without removing them entirely.                              |\n",
    "| Robust Scaling               | Scale data using robust statistics like Median Absolute Deviation (MAD) to make it less sensitive to outliers.                                                                | When you need to scale data while considering the presence of outliers.                                        |\n",
    "| Outlier Imputation           | Replace outlier values with more typical values based on data characteristics.                                                                                              | When you want to maintain the integrity of the dataset while minimizing the impact of outliers.               |\n",
    "| Non-Parametric Methods       | Use algorithms like decision trees and random forests that are robust to outliers and can handle skewed data effectively.                                                    | When dealing with complex relationships and interactions in the data.                                         |\n",
    "| Bucketing or Binning         | Group data into bins or buckets to represent skewed data in a more balanced and meaningful way.                                                                              | When dealing with continuous data with a wide range and you want to create more interpretable features.       |\n",
    "| Transformation Combinations | Combine multiple transformations (e.g., log transformation and scaling) to address both skewness and outliers.                                                                | When data exhibits both skewness and outliers, and simple approaches are not sufficient.                      |\n",
    "| Remove Outliers              | Remove extreme values from the dataset altogether.                                                                                                                            | When outliers have a significant impact on the analysis or modeling task and are likely to be erroneous data. |\n",
    "\n",
    "Use these methods based on the characteristics of your data and the goals of your analysis or modeling. There is no one \"best\" method for all scenarios, and you may need to experiment with different techniques to find the most suitable approach for your specific case. Additionally, consider consulting domain experts or experienced data scientists to get valuable insights and guidance in selecting the appropriate method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Techniques \n",
    "Here's a table with the most efficient and commonly used techniques in various data science tasks:\n",
    "\n",
    "| Task                  | Technique                                             | Code (where applicable)                            |\n",
    "|-----------------------|-------------------------------------------------------|----------------------------------------------------|\n",
    "| Missing Value Imputation | Multiple Imputation                                   | N/A (requires specific library implementations)   |\n",
    "| Outlier Detection     | Z-Score method                                        | ```from scipy import stats```<br>```z_scores = stats.zscore(df['numeric_column'])``` |\n",
    "|                       | Interquartile Range (IQR) method                      | ```Q1 = df['numeric_column'].quantile(0.25)```<br>```Q3 = df['numeric_column'].quantile(0.75)```<br>```IQR = Q3 - Q1```<br>```outliers = df[(df['numeric_column'] < (Q1 - 1.5 * IQR)) | (df['numeric_column'] > (Q3 + 1.5 * IQR))]``` |\n",
    "| Feature Scaling       | Standardization (Z-Score scaling)                    | ```from sklearn.preprocessing import StandardScaler```<br>```scaler = StandardScaler()```<br>```scaled_data = scaler.fit_transform(df[['numeric_column']])``` |\n",
    "|                       | Min-Max scaling                                      | ```from sklearn.preprocessing import MinMaxScaler```<br>```scaler = MinMaxScaler()```<br>```scaled_data = scaler.fit_transform(df[['numeric_column']])``` |\n",
    "| Dimensionality Reduction | Principal Component Analysis (PCA)                  | ```from sklearn.decomposition import PCA```<br>```pca = PCA(n_components=2)```<br>```principal_components = pca.fit_transform(df[['feature1', 'feature2']])``` |\n",
    "| Feature Selection     | Tree-Based Feature Importance                        | ```from sklearn.ensemble import RandomForestClassifier```<br>```model = RandomForestClassifier()```<br>```model.fit(X_train, y_train)```<br>```feature_importance = model.feature_importances_``` |\n",
    "| Model Evaluation      | Cross-Validation                                     | ```from sklearn.model_selection import cross_val_score```<br>```scores = cross_val_score(model, X, y, cv=5)``` |\n",
    "|                       | Grid Search and Hyperparameter Tuning                | ```from sklearn.model_selection import GridSearchCV```<br>```param_grid = {'param1': [value1, value2], 'param2': [value3, value4]}```<br>```grid_search = GridSearchCV(model, param_grid, cv=5)```<br>```grid_search.fit(X_train, y_train)``` |\n",
    "|                       | Model Performance Metrics (e.g., accuracy, precision, recall, F1-score) | ```from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score```<br>```y_pred = model.predict(X_test)```<br>```accuracy = accuracy_score(y_test, y_pred)```<br>```precision = precision_score(y_test, y_pred)```<br>```recall = recall_score(y_test, y_pred)```<br>```f1 = f1_score(y_test, y_pred)``` |\n",
    "| Best Algorithm       | XGBoost                                              | ```from xgboost import XGBClassifier```<br>```model = XGBClassifier()```<br>```model.fit(X_train, y_train)```<br>```y_pred = model.predict(X_test)``` |\n",
    "\n",
    "These are some of the most efficient and commonly used techniques in data science tasks. Depending on your specific problem and dataset, you may need to fine-tune these techniques or combine them to achieve the best results. Always consider the context and requirements of your project when selecting the appropriate methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
